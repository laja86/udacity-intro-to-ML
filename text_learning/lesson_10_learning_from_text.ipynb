{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "__definition:__ deprecated vs. obsolete\n",
    "- _deprecated_: a status applied to software features to indicate that they should be avoided, typically because they have been superseded. \n",
    "    - Features are rather deprecated rather than removed in order to provide backward compatibility, giving programmers who have used the feature time to bring their code into compliance with the new standard.\n",
    "- _obsolete_: Obsolete features are those deemed redundant and are highly likely to be discontinued in the next few releases.\n",
    "\n",
    "### Useful Python 2 Code Snippets\n",
    "- `raise SystemExit(0)` used to stop python script (like break but outside loop)\n",
    "\n",
    "\n",
    "### Techniques\n",
    "`temp_counter` applied in vectorize_text.py, in original file.\n",
    "- limits processing to subset of dataset\n",
    "- useful in development stage so modifications can be done more quickly\n",
    "- remove when working to run through whole dataset\n",
    "\n",
    "### IPython functions\n",
    "#### Exploring accessible variables:\n",
    "- `dir()` lists scope variables:\n",
    "- `globals()` gives a dictionary of global variables\n",
    "- `locals()` gives a dictionary of local variables\n",
    "- `whos` lists currently defined variable (or `who` for less detail)\n",
    "- `sys.modules` is a dictionary variable that maps all names of all currently loaded modules to module objects.\n",
    "    - The contents of this dictionary are used to determine whether import loads a fresh copy of a module (reimporting a module does not rerun the code, just references namespace).\n",
    "    - See [Python Essential Reference 4th Edition by David M. Beazley (2009)](https://www.evernote.com/shard/s37/nl/1033921335/e9944533-f7e9-4614-b5aa-41dbe8aa9900/) for more on modules and packages (chapter 8)\n",
    "\n",
    "Applying built-in function `vars`([_object_]):\n",
    "- Potentially very useful, needs some filtering/formating\n",
    "- `vars()` without an argument behaves like `locals()`\n",
    "    - only useful for reads, updates to returned dictionary are ignored\n",
    "    - also takes an optional argument to find out which vars are defined within an object itself\n",
    "- To get the names :\n",
    "    - ```for name in vars().keys():\n",
    "      print(name)```\n",
    "- To get the values:\n",
    "    - ```for value in vars().values():\n",
    "      print(value)```\n",
    "\n",
    "\n",
    "### String Functions (deprecated in Python 2)\n",
    "Used in `../tools/parse_out_email_text.py` for quizes below\n",
    "[Python 2 documentation](https://docs.python.org/2/library/string.html#string-functions)\n",
    "- [Extra: Python 3 documentation](https://docs.python.org/3.1/library/string.html)\n",
    "- always true that string.`join(string.split(s, sep), sep)` equals _s_.\n",
    "- `string.translate`(s, table[, deletechars])\n",
    "    - Delete all characters from s that are in deletechars (if present), and then translate the characters using table, which must be a 256-character string giving the translation for each character value, indexed by its ordinal. If table is None, then only the character deletion step is performed.\n",
    "- `string.translate`(s, table[, deletechars])\n",
    "    - Delete all characters from s that are in deletechars (if present), and then translate the characters using table, which must be a 256-character string giving the translation for each character value, indexed by its ordinal. If table is None, then only the character deletion step is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions when Learning From Text\n",
    "- Length of text is variable, so input dimensions will vary too\n",
    "- Solution: use dictionary with word : frequency (bag of words)\n",
    "\n",
    "### Bag of Words\n",
    "Properties\n",
    "- Does not consider word order data (hence \"bag\")\n",
    "- Count value means longer phrases give different input vectors\n",
    "    - \"Biasing to encode the length of the text\"\n",
    "- Can only handle complex phrases unless extra bags are created for them\n",
    "    - e.g. classic example of complex phrase is \"chicago bulls\" vs \"chicago\" + \"bulls\"\n",
    "\n",
    "### Sklearn - Bag of Words\n",
    "#### [`CountVectorizer()`][alg]\n",
    "`vectorizer.fit_transform()`\n",
    "- returns `dict` of `(document_index, word_position) : freq` mapping\n",
    "- `.fit()` makes some sort of indexed list of all words\n",
    "- `.transform()` assigns a count to each of the words\n",
    "- `vectorizer.vocabulary_.get(\"word\")` gets feature number of word\n",
    "    from `vocabulary_` attribute\n",
    "\n",
    "[User Guide: 4.2.3. Text feature extraction][user guide]\n",
    "\n",
    "Extra [Tutorial: Working With Text Data][tutorial]\n",
    "\n",
    "[alg]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "[user guide]: http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "[tutorial]: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "### `nltk` corpus/vocabulary library\n",
    "[NLTK 3.0 Documentation](http://www.nltk.org/)\n",
    "- NOTE: Need to download corpi (corpuses?) first with `nltk.download()`\n",
    "\n",
    "##### Stopwords\n",
    "- _Definition_: low information, high frequency\n",
    "- Examples of low-info words from quiz: the, will, hi, names (since emails only from two people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quiz: how many English stopwords?\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "# sw is a list, ordered by most common usage to least\n",
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming\n",
    "- Not all unique words have different meanings\n",
    "- Stemmers are functions the take words with similar meanings and group them into one-dimensional feature\n",
    "    - e.g. response, responsive, respond... -> respon\n",
    "- computational linguists make these functions\n",
    "\n",
    "[`nltk.stem.snowball.`**`SnowballStemmer()`**][doc] module documentation\n",
    "- An example stemmer in `nltk`\n",
    "\n",
    "[doc]: http://www.nltk.org/api/nltk.stem.html?highlight=snowballstemmer#nltk.stem.snowball.SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('responsiveness'))  # = respons\n",
    "print(stemmer.stem('unresponsive'))  # = unrespons\n",
    "# this difference in output is a potential limitation\n",
    "#      might not be what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of Operations in Text Processing\n",
    "1. Stemming\n",
    "2. Bag-of-words representation\n",
    "\n",
    "This will affect bag-of-words output, but is also more practical because of bag-of-words datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Words (features)\n",
    "#### Tf vs. Idf representations\n",
    "- _Term frequency (Tf)_: like bag-of-words representation, weigthing words by frequency\n",
    "- _Inverse document frequency (Idf)_: weighting by frequency of words in corpus as a whole\n",
    "    - weights rare words higher than more common words; rare words better distinguish differences between documents\n",
    "    - covered in upcoming mini-project\n",
    "    \n",
    "[`sklearn.feature_extraction.text.`**`TfidfVectorizer`**][doc] module documentation\n",
    "- Combines all the options of CountVectorizer and TfidfTransformer in a single model\n",
    "\n",
    "[4.2.3. Text feature extraction][userguide] user guide linked in documentation\n",
    "- \n",
    "\n",
    "[doc]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "[userguide]: http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Project! on Preprocessing & Text Learning\n",
    "In the beginning of this class, you identified emails by their authors using a number of supervised classification algorithms. In those projects, we handled the preprocessing for you, transforming the input emails into a TfIdf so they could be fed into the algorithms. Now you will construct your own version of that preprocessing step, so that you are going directly from raw data to processed features.\n",
    "\n",
    "#### Quiz: Warm up\n",
    "- You will be given two text files: one contains the locations of all the emails from Sara, the other has emails from Chris. You will also have access to the parseOutText() function, which accepts an opened email as an argument and returns a string containing all the (stemmed) words in the email.\n",
    "- You’ll start with a warmup exercise to get acquainted with parseOutText(). Go to the tools directory and run parse_out_email_text.py, which contains parseOutText() and a test email to run this function over.\n",
    "- __parseOutText()__ takes the opened email and returns only the text part, stripping away any metadata that may occur at the beginning of the email, so what's left is the text of the message. We currently have this script set up so that it will print the text of the email to the screen, what is the text that you get when you run parseOutText()?\n",
    "- _A hint when submitting_: the words in the string that you get have TWO spaces between them; make sure your answer does too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# atom /Users/mdlynch37/Documents/udacity/intro_to_ml/ud120-projects/tools/parse_out_email_text.py\n",
    "%run ../tools/parse_out_email_text.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Out [ ]: Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Deploying Stemming\n",
    "- In parseOutText(), comment out the following line: `words = text_string`\n",
    "- Augment parseOutText() so that the string it returns has all the words stemmed using a SnowballStemmer.\n",
    "- Use the nltk package, some examples that I found helpful can be found in the [nltk howto guide](http://www.nltk.org/howto/stem.html). Rerun parse_out_email_text.py, which will use your updated parseOutText() function--what’s your output now?\n",
    "    - Hint: you'll need to break the string down into individual words, stem each word, then recombine all the words into one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\r\n"
     ]
    }
   ],
   "source": [
    "!python2 ../tools/parse_out_email_text.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Out [ ]: hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Quiz: Clean Away \"Signature Words\"\n",
    "- In vectorize_text.py, you will iterate through all the emails from Chris and from Sara. \n",
    "- For each email, feed the opened email to parseOutText() and return the stemmed text string.\n",
    "- Then do two things:\n",
    "    1. remove signature words (“sara”, “shackleton”, “chris”, “germani”--bonus points if you can figure out why it's \"germani\" and not \"germany\")\n",
    "    - append the updated text string to word_data -- if the email is from Sara, append 0 (zero) to from_data, or append a 1 if Chris wrote the email.\n",
    "- Once this step is complete, you should have two lists: one contains the stemmed text of each email, and the second should contain the labels that encode (via a 0 or 1) who the author of that email is.\n",
    "- Running over all the emails can take a little while (5 minutes or more), so we've added a temp_counter to cut things off after the first 200 emails. Of course, once everything is working, you'd want to run over the full dataset.\n",
    "\n",
    "In the box below, put the string that you get for word_data[152]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tjonesnsf stephani and sam need nymex calendar\n"
     ]
    }
   ],
   "source": [
    "import vectorize_text as vt\n",
    "print vt.word_data[152]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Out [ ]:  tjonesnsf stephani and sam need nymex calendar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: TfIdf It\n",
    "- Make sure to first remove english stopwords and disable temp_counter\n",
    "- Transform the word_data into a tf-idf matrix using the sklearn TfIdf transformation.\n",
    "    - Use tf-idf Vectorizer class to transform the word data.\n",
    "- You can access the mapping between words and feature numbers using `get_feature_names()`, which returns a list of all the words in the vocabulary. How many different words are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n",
      "word data vectorized with method 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38757"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if statement used to determine if module had previous been imported\n",
    "import sys\n",
    "if 'vectorize_text' in sys.modules.keys():\n",
    "    vt = reload(vt)\n",
    "else:\n",
    "    import vectorize_text as vt\n",
    "feature_names = vt.vectorizer.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Out [ ]: 38757`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Accessing TfIdf Features\n",
    "- What is word number 34597 in your TfIdf?\n",
    "    - (Just to be clear--if the question were \"what is word number 100,\" we would be looking for the word corresponding to vocab_list[100]. Zero-indexed arrays are so confusing to talk about sometimes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'stephaniethank'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[34597]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Out [ ]: u'stephaniethank'`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
