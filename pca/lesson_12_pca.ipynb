{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Note on data structure: list\n",
    "    - empty list has a truth value of false\n",
    "- [Feature Selection with scikit-learn for intro_to_ml](http://napitupulu-jon.appspot.com/posts/feature-selection-ud120.html)\n",
    "    - Looks very helpful for copying notes, course materials\n",
    "    - Investigate meaning of `# %%writefile new_enron_feature.py` inserted at top of edited studentMain.py module\n",
    "\n",
    "### ML Algorithms\n",
    "- A classic way to overfit an algorithm is by using lots of features and not a lot of training data.\n",
    "- _Decision Trees_ are easy to overfit.\n",
    "- classic use of regression is when output/labels consists of continuous data (e.g. from features of house determine its price)\n",
    "\n",
    "### Useful git code snippets\n",
    "- `git reset --soft HEAD~`\n",
    "    - Leaves working tree as it was before git commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principlal Component Analysis\n",
    "`sklearn.decomposition`.PCA [Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [User Guide](http://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- `n_components == min(n_samples, n_features)`\n",
    "- `explained_variance_ratio_` list of eigenvalues for each pricipal component (adds up to 1)\n",
    "- `components_` list of principal components, provided directional information of components\n",
    "- note that visualization might not seem to show orthogonal lines, but this is because of how the scale is done (could cut off based on lower limit)\n",
    "\n",
    "__definition:__ principal component analysis (PCA)\n",
    "- PCA returns straight-line _axes of variation_ as vectors, as well as an importance value for each one\n",
    "    - These two axes define a _coordinate system_ centered around the data.\n",
    "    - the the x-prime vector (like x-axis) is aligned with the _principal axis of variation_ (similar to like regression line, higher importance value of the two)\n",
    "    - the y-prime is vector orthogonal to x-prime (dot product would equal 0)\n",
    "- Part of it's beauty is that it can be useful with data not perfectly 1D, i.e. not well fit to a regression line.\n",
    "- since PCA uses vectors for axis, more versatile than regression y = f(x) with x = c cases (swaps axes)\n",
    "- Importance value\n",
    "    - calculated with an _eigenvalue decomposition_ implemented by PCA (math, will learn later/as needed)\n",
    "    - If x-axis _dominates_ y-axis, that means it has a much higher importance value\n",
    "    - If no axis dominates, PCA output not useful\n",
    "- ![PCA data set examples](lesson_12_images/pca_datasets.png)\n",
    "\n",
    "## Dimensionality in PCA\n",
    "- Examples of one-dimensional data that exist in two-dimensional space, as defined in PCA:\n",
    "    - y = c and x = c (even with noise)\n",
    "    - straight diagonal lines\n",
    "        - appies even when there are small deviations (noise)\n",
    "        - can manipulate (_by rotation and translation only_) with x-prime  and y-prime notation for new axes\n",
    "- Curved lines of data that can be manipulated into 1D representations (like for regressions) are _not_ considered 1D when using PCA.\n",
    "     - ![Exmaple of dimensionality for PCA](lesson_12_images/pca_dimensionality.png)\n",
    "\n",
    "## Simple Examples:\n",
    "- PCA outputs vectors that are normalized to 1\n",
    "- Orthogonal vectors being 1 / (root of two)\n",
    "    - those are x-prime and y-prime, each consisting of a delta-x and delta-y\n",
    "\n",
    "Example 1:\n",
    "![PCA Example 1](lesson_12_images/pca_example_1.png)\n",
    "\n",
    "Example 2:\n",
    "- ++ in image below indicates that the x-prime axis of variation will have much higher important than the other.\n",
    "![PCA Example 2](lesson_12_images/pca_example_2.png)\n",
    "\n",
    "## Measurable vs. Latent Features\n",
    "- folds measurable features into single latent feature (an underlying factor we can determine form intuition)\n",
    "\n",
    "    - e.g. no. of rooms, square-footage of house -> size of house\n",
    "    - e.g. safety of neighborhood, schools nearby -> neighborhood\n",
    "- Can use SelectKBest (or maybe SelectPercentile) to preserve data, but fold into latent aspects\n",
    "\n",
    "## Composite Features\n",
    "- Can make a composite feature (or principle component from PCA!) to measure/represent latent feature\n",
    "    - part of dimensionality reduction and unsupervised learning (covered later in course)\n",
    "    \n",
    "    \n",
    "## Determining a Principle Component\n",
    "### Maximal Variance\n",
    "def \n",
    "- seeks to minimize information loss when translating into 1D\n",
    "- information lost is proportional to distance from point to line\n",
    "- direction of maximal variance is mathematically defined as line that has least information loss (in aggregate, for all data points)\n",
    "\n",
    "From wiki: (confusing that principle is described as having _higher_ variance of the two components). key: _direction_ of maximal variable, not line with most variance.\n",
    "This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "### Maximal Variance and Information Loss\n",
    "\n",
    "\n",
    "\n",
    "### PCA as a Generalized Algorithm for Feature Transformation\n",
    "- necessary for scale\n",
    "- PCA algorithm will run through all combinations and provide first principal component, second, etc. ranked by importance value\n",
    "- powerful unsupervised learning technique\n",
    "\n",
    "### When to Use PCA\n",
    "1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PCA Mini-Project!\n",
    "- Eisenfaces code mostly taken from [this example](http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html) from sklearn's documentation.\n",
    "\n",
    "We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?\n",
    "\n",
    "- _Answer:_ 0.17561573, 0.15863393 (not accepting it, different people got different answers depending on OS, sklearn version, etc.)\n",
    "\n",
    "Visual Output:\n",
    "\n",
    "![eigenfaces visual output](lesson_12_images/eigenfaces_visual_output.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================\n",
      "Faces recognition example using eigenfaces and SVMs\n",
      "===================================================\n",
      "\n",
      "The dataset used in this example is a preprocessed excerpt of the\n",
      "\"Labeled Faces in the Wild\", aka LFW_:\n",
      "\n",
      "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
      "\n",
      "  .. _LFW: http://vis-www.cs.umass.edu/lfw/\n",
      "\n",
      "  original source: http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html\n",
      "\n",
      "\n",
      "Total dataset size:\n",
      "n_samples: 1217\n",
      "n_features: 1850\n",
      "n_classes: 6\n",
      "Extracting the top 15 eigenfaces from 912 faces\n",
      "done in 0.060s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.011s\n",
      "Fitting the classifier to the training set\n",
      "done in 11.164s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Predicting the people names on the testing set\n",
      "done in 0.008s\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.29      0.36      0.32        14\n",
      "     Colin Powell       0.70      0.62      0.66        65\n",
      "  Donald Rumsfeld       0.64      0.48      0.55        33\n",
      "    George W Bush       0.77      0.83      0.80       133\n",
      "Gerhard Schroeder       0.46      0.52      0.49        23\n",
      "       Tony Blair       0.51      0.51      0.51        37\n",
      "\n",
      "      avg / total       0.66      0.66      0.66       305\n",
      "\n",
      "[[  5   3   0   4   2   0]\n",
      " [  7  40   3  10   0   5]\n",
      " [  3   2  16   5   4   3]\n",
      " [  2  10   2 110   6   3]\n",
      " [  0   0   1   3  12   7]\n",
      " [  0   2   3  11   2  19]]\n",
      "\n",
      "Q1: [ 0.17561573  0.15863393]\n"
     ]
    }
   ],
   "source": [
    "from eigenfaces import *\n",
    "# index given slightly differently from list\n",
    "print\n",
    "print \"Q1:\", pca.explained_variance_ratio_[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll experiment with keeping different numbers of principal components. In a multiclass classification problem like this one (more than 2 labels to apply), accuracy is a less-intuitive metric than in the 2-class case. Instead, a popular metric is the F1 score.\n",
    "\n",
    "We’ll learn about the F1 score properly in the lesson on evaluation metrics, but you’ll figure out for yourself whether a good classifier is characterized by a high or low F1 score. You’ll do this by varying the number of principal components and watching how the F1 score changes in response.\n",
    "\n",
    "As you add more principal components as features for training your classifier, do you expect it to get better or worse performance?\n",
    "\n",
    "- _Answer:_ Better. Ideally, we hope that adding more components will give us more signal information to improve the classifier performance.\n",
    "\n",
    "Change n_components to the following values: [10, 15, 25, 50, 100, 250]. For each number of principal components, note the F1 score for Ariel Sharon. (For 10 PCs, the plotting functions in the code will break, but you should be able to see the F1 scores.) If you see a higher F1 score, does it mean the classifier is doing better, or worse?\n",
    "\n",
    "- _Answer:_ Better. Higher F1 means better performance of classifier.\n",
    "\n",
    "Do you see any evidence of overfitting when using a large number of PCs? Does the dimensionality reduction of PCA seem to be helping your performance here?\n",
    "\n",
    "- _Answer:_ Yes, the F1 score starts to drop when there are too many PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Number of Principle Components\n",
    "- Best way of determining this is by testing different no. of components\n",
    "    - like when determining which features to include, from ranked importance/relevance\n",
    "- Note: Do PCA _before_ feature selection (otherwise proceed with caution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
