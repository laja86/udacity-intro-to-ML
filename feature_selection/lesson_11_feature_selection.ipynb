{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Note on data structure: list\n",
    "    - empty list has a truth value of false\n",
    "- [Feature Selection with scikit-learn for intro_to_ml](http://napitupulu-jon.appspot.com/posts/feature-selection-ud120.html)\n",
    "    - Looks very helpful for copying notes, course materials\n",
    "    - Investigate meaning of `# %%writefile new_enron_feature.py` inserted at top of edited studentMain.py module\n",
    "\n",
    "### ML Algorithms\n",
    "- A classic way to overfit an algorithm is by using lots of features and not a lot of training data.\n",
    "- _Decision Trees_ are easy to overfit.\n",
    "\n",
    "### Python 2\n",
    "```python\n",
    "### there can be many \"to\" emails, but only one \"from\", so the\n",
    "### \"to\" processing needs to be a little more complicated\n",
    "# uses counter for iterating through, duplicates process for cc_emails\n",
    "#   does not seem very pythonic, but maybe clearest method\n",
    "if to_emails:\n",
    "    ctr = 0  # counter for iterating through, perhaps not pythonic\n",
    "    while not to_poi and ctr < len(to_emails):\n",
    "        if to_emails[ctr] in poi_email_list:\n",
    "            to_poi = True\n",
    "        ctr += 1\n",
    "```\n",
    "\n",
    "### Useful git code snippets\n",
    "- `git reset --soft HEAD~`\n",
    "    - Leaves working tree as it was before git commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Selection?\n",
    "\"Make everything as simple as possible, but no simpler\" - Albert Einstein\n",
    "\n",
    "Two major things aspects:\n",
    "1. Select best features, leaving out unecessary data\n",
    "- Adding new features to explore data, using intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of exploring data with Enron corpus example\n",
    "- Quiz 1: Coding up a new feature\n",
    "- Quiz 2: Scaling this feature\n",
    "\n",
    "### Steps in Katie's process:\n",
    "- Use her human intuition\n",
    "    - Quiz 1: POIs might send other POIs emails more often than the general population\n",
    "    - Quiz 2: Feature scaling might provide more useful visualization\n",
    "- Code up the new feature \n",
    "    - Quiz 1: Calculate aggregate of emails sent from POIs to each person\n",
    "    - Quiz 2: Scale by total number of messages to and from that person\n",
    "- Visualize\n",
    "    - Quiz 1: See scatter plot below to check whether feature gives discriminating power between POIs and non-POIs.\n",
    "- Repeat\n",
    "    - Improve preceeding parts of process\n",
    "    - Zero in on the feature that would be most helpful\n",
    "    \n",
    "__Visualizations:__\n",
    "\n",
    "_Quiz 1:_ Not very useful without scaling\n",
    "![quiz1 scatterplot](lesson_11_images/quiz1_scatter.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a buggy feature\n",
    "When Katie was working on the Enron POI identifier, she engineered a feature that identified when a given person was on the same email as a POI. So for example, if Ken Lay and Katie Malone are both recipients of the same email message, then Katie Malone should have her \"shared receipt\" feature incremented. If she shares lots of emails with POIs, maybe she's a POI herself.\n",
    "\n",
    "Here's the problem: there was a subtle bug, that Ken Lay's \"shared receipt\" counter would also be incremented when this happens. And of course, then Ken Lay always shares receipt with a POI, because he is a POI. So the \"shared receipt\" feature became extremely powerful in finding POIs, because it effectively was encoding the label for each person as a feature.\n",
    "\n",
    "We found this first by being suspicious of a classifier that was always returning 100% accuracy. Then we removed features one at a time, and found that this feature was driving all the performance. Then, digging back through the feature code, we found the bug outlined above. We changed the code so that a person's \"shared receipt\" feature was only incremented if there was a different POI who received the email, reran the code, and tried again. The accuracy dropped to a more reasonable level.\n",
    "\n",
    "We take a couple of lessons from this:\n",
    "- Anyone can make mistakes--be skeptical of your results!\n",
    "- 100% accuracy should generally make you suspicious. Extraordinary claims require extraordinary proof.\n",
    "- If there's a feature that tracks your labels a little too closely, it's very likely a bug!\n",
    "- If you're sure it's not a bug, you probably don't need machine learning--you can just use that feature alone to assign labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring features\n",
    "### Reasons to ignore a feature\n",
    "- Too much noise, hard to dinstinguish whether it is reliably measuring what you want it to be measuring, i.e. data is not accurate/reliable enough.\n",
    "- Causes overfitting for some reason\n",
    "- Highly correlated with/strong related to a feature that is already present, breaking the model because mathematics stops working.\n",
    "- Unecessarily slows down training/testing process when feature is clearly not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features != Information\n",
    "__definition:__ Features vs. Information\n",
    "- A feature is a characteristic of particular data point that is attempting to _access_ information\n",
    "    - In general, we want bare minimum number of features that give as much information as possible\n",
    "- Can think of features as quantity vs. information as quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection tools in sklearn\n",
    "- Feature reduction a.k.a dimensionality reduction\n",
    "- Very important to be skeptical of features, esp. with high _dimensionality data_\n",
    "- In example in tools/email_preprocess.py described below, 90% of features we ignored with:\n",
    "    - insignificant impact on the classifier's accuracy, and \n",
    "    - performance improved in terms of the _time complexity_ of the classifier algorithm.\n",
    "\n",
    "### Univariate Feature Selection\n",
    "There are several go-to methods of automatically selecting your features in sklearn. Many of them fall under the umbrella of univariate feature selection, which treats each feature independently and asks how much _power_ it gives you in classifying or regressing.\n",
    "\n",
    "There are two big univariate feature selection tools in sklearn:\n",
    "- `SelectPercentile` and `SelectKBest`. \n",
    "- The difference is pretty apparent by the names:\n",
    "    - SelectPercentile selects the X% of features that are most powerful (where X is a parameter)\n",
    "    - SelectKBest selects K number of features that are most powerful (where K is a parameter).\n",
    "\n",
    "A clear candidate for feature reduction is text learning, since _the data has such high dimension_. We actually did feature selection in the Sara/Chris email classification problem during the first few mini-projects; you can see it in the code in tools/email_preprocess.py:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "...\n",
    "...\n",
    "### feature selection, because text is super high dimensional and \n",
    "### can be really computationally chewy as a result\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features_train_transformed, labels_train)\n",
    "features_train_transformed = selector.transform(\n",
    "    features_train_transformed).toarray()\n",
    "features_test_transformed  = selector.transform(\n",
    "    features_test_transformed).toarray()\n",
    "```\n",
    "\n",
    "### Feature Selection in TfIdf Vectorizer\n",
    "- NOTE: Usually not a good idea to mix univariate feature selection and Feature Selection with TfIdf Vectorizer parameter `max_df`\n",
    "\n",
    "Example from tools/email_preprocess.py:\n",
    "```python\n",
    "### text vectorization--go from strings to lists of numbers\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                               stop_words='english')\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "features_test_transformed  = vectorizer.transform(features_test)\n",
    "```\n",
    "\"`max_df=0.5`\" parameter means that words with a document frequency of more than 0.5 will be removed.\n",
    "- i.e. words that occur in more than 50% of the documents are not included as features\n",
    "- used because words that are probably too common to be very 'powerful', does not provide 'access' to information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, And Number Of Features\n",
    "__definition:__ Bias vs. Variance\n",
    "- High bias algorithm pays _too little_ attention to data, not effected much by data (i.e. oversimplified)\n",
    "    - characterized by _high error_ on training set, i.e. low r<sup>2</sup> / large SSE or sum of squared (residual) errors\n",
    "    - _common when:_ too few features used in model\n",
    "- High variance algorithm pays _too much_ attention to data, does not generalize well, i.e. it overfits to the data\n",
    "    - characterized by _much higher error_ on test set than training (some variance between the two is expected).\n",
    "    - _common when:_ carefully minimized SSE, with too many features i.e. overfit to data\n",
    "\n",
    "__trade-off:__ Quality of model vs. no. of features:\n",
    "\n",
    "Goal is to balance:\n",
    "- the performance/accuracy of the model on the training data\n",
    "    - (without overfitting)\n",
    "- with as few features as possible.\n",
    "    - (while maintaining algorithm's performance)\n",
    "    \n",
    "### Visualizing Overfitting\n",
    "#### An Overfit Regression\n",
    "- Blue points are training data\n",
    "- Red points are test data\n",
    "![An Overfit Regression](lesson_11_images/overfit_regression.png)\n",
    "\n",
    "### Regularization\n",
    "__definition:__ automatically penalizing extra features in model\n",
    "\n",
    "![Regularization graph](lesson_11_images/regularization_graph.png)\n",
    "\n",
    "#### Lasso Regression (type of regularized regression)\n",
    "`sklearn.linear_model`.Lasso [Documentation][lasso_doc] and [User Guide][lasso_user]\n",
    "- Mathematical optimization of the bias-variance trade-off\n",
    "- Minimizes SSE like basic regression\n",
    "- But also minimizes term (penalty parameter * coefficients of regression)\n",
    "    - coefficients of regression describe/are related to no. of features\n",
    "    - if feature does not add enough precision, it's coefficient is set to 0\n",
    "        - simpler algorithm with this method since it can run through the formula in-place.\n",
    "- This means that any loss from an additional feature must be offset by the gain in precision by adding that feature.\n",
    "\n",
    "   \n",
    "![Lasso regression formula](lesson_11_images/lasso_regression_formula.png)\n",
    "![Lasso regression penalty method](lesson_11_images/lasso_regression_penalty_method.png)\n",
    "\n",
    "[lasso_doc]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "[lasso_user]: http://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-project! on feature selection\n",
    "Katie explained in a video a problem that arose in preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here.\n",
    "\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.\n",
    "If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?\n",
    "\n",
    "- _Answer:_ Pretty low (accuracy on training set would be way higher)\n",
    "\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. How many training points are there, according to the starter code?\n",
    "- _Answer:_ 150\n",
    "- Special Note: if you are having trouble getting the code to run due to memory issues, then if you are on version 0.16.x of scikit-learn, you can remove the .toarray() function from the line where features_train is created to save on memory - the decision tree classifier can, in that version take as input a sparse array instead of only dense arrays.\n",
    "\n",
    "What’s the accuracy of the decision tree you just made? (Remember, we're setting up our decision tree to overfit -- ideally, we want to see the test accuracy as relatively low.)\n",
    "- _Answer:_ 0.947667804323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950511945392\n",
      "[(0.66666666666666674, 14343), (0.16260162601626021, 8674)]\n",
      "cgermannsf\n",
      "0.970420932878\n"
     ]
    }
   ],
   "source": [
    "from find_signature import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# same as print clf.score(features_test, labels_test)\n",
    "pred = clf.predict(features_test)\n",
    "print accuracy_score(labels_test, pred)\n",
    "#    out 1: 0.947667804323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Most Powerful Features\n",
    "Take your (overfit) decision tree and use the `feature_importances` attribute to get a list of the relative importance of all the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01). \n",
    "\n",
    "What’s the importance of the most important feature?\n",
    "- _Answer:_ 0.76470588235294124\n",
    "\n",
    "What is the number of this feature?\n",
    "- _Answer:_ 33614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5924f8918774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mIMP_THRESH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m  \u001b[0;31m# importance threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m imp_features = [(j, i) for i, j\n\u001b[1;32m      4\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 if j > IMP_THRESH]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "IMP_THRESH = 0.1  # importance threshold\n",
    "feature_importances = clf.feature_importances_\n",
    "imp_features = [(j, i) for i, j\n",
    "                in enumerate(feature_importances) \n",
    "                if j > IMP_THRESH]\n",
    "imp_features.sort(reverse=True)\n",
    "print imp_features\n",
    "#    out 1: [(0.76470588235294124, 33614)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the TfIdf by calling get_feature_names() on it; pull out the word that’s causing most of the discrimination of the decision tree. \n",
    "\n",
    "What is it?\n",
    "- _Answer:_ \n",
    "\n",
    "Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?\n",
    "- _Answer:_ No! This is her email address or something.\n",
    "- _Even though our training data is limited, we still have a word that is highly indicative of author._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshacklensf\n"
     ]
    }
   ],
   "source": [
    "all_features = vectorizer.get_feature_names()\n",
    "problem_index = imp_features[0][1]\n",
    "problem_word = all_features[problem_index]\n",
    "print problem_word\n",
    "#   out 1: sshacklensf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, and once that finishes, rerun find_signature.py.\n",
    "\n",
    "Any other outliers pop up? What word is it?\n",
    "- cgermannsf\n",
    "\n",
    "Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before).\n",
    "- Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.816837315131\n",
      "\n",
      "Features ranked by importance value:\n",
      "[(0.36363636363636365, 21323), (0.18692724344898259, 18849), (0.10537857900318125, 11975)]\n",
      "Potential outlier word:\n",
      "houectect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Had to rerun vectorize_text in iPython interpreter\n",
    "# import sys\n",
    "# sys.path.append( \"../tools/\" )\n",
    "# import vectorize_text\n",
    "#    returns: \n",
    "#         IOError: [Errno 2] No such file or directory: 'from_sara.txt'\n",
    "\n",
    "# previous code cells added to find_signature module\n",
    "import sys\n",
    "if 'find_signature' in sys.modules.keys():\n",
    "    find_signature = reload(find_signature)\n",
    "else:\n",
    "    from find_signature\n",
    "from find_signature import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update vectorize_text.py one more time, and rerun. Then run find_signature.py again. \n",
    "\n",
    "Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?\n",
    "- _Answer:_ Yes, there is one more word (\"houectect\").  Your guess about what this word means is as good as ours, but it doesn't look like an obvious signature word so let's keep moving without removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:\n",
      "0.816837315131\n",
      "\n",
      "Words in order of importance value:\n",
      "[('houectect', 0.36363636363636365), ('fax', 0.18692724344898259), ('attach', 0.10537857900318125)]\n"
     ]
    }
   ],
   "source": [
    "# updated file for better output\n",
    "import sys\n",
    "if 'find_signature' in sys.modules.keys():\n",
    "    find_signature = reload(find_signature)\n",
    "else:\n",
    "    from find_signature import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Output:_\n",
    "```\n",
    "Accuracy score:\n",
    "0.816837315131\n",
    "\n",
    "Words in order of importance value:\n",
    "[('houectect', 0.36363636363636365)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s the accuracy of the decision tree now? We've removed two \"signature words\", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!\n",
    "\n",
    "What’s the accuracy of the decision tree now? \n",
    "- _Answer:_ 0.816837315131\n",
    "\n",
    "Excellent work! Now that we've removed the outlier \"signature words\", the training data is starting to overfit to the words that remain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
