{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Note on data structure: list\n",
    "    - empty list has a truth value of false\n",
    "- [Feature Selection with scikit-learn for intro_to_ml](http://napitupulu-jon.appspot.com/posts/feature-selection-ud120.html)\n",
    "    - Looks very helpful for copying notes, course materials\n",
    "    - Investigate meaning of `# %%writefile new_enron_feature.py` inserted at top of edited studentMain.py module\n",
    "\n",
    "### ML Algorithms\n",
    "- A classic way to overfit an algorithm is by using lots of features and not a lot of training data.\n",
    "- _Decision Trees_ are easy to overfit.\n",
    "\n",
    "### Python 2\n",
    "```python\n",
    "### there can be many \"to\" emails, but only one \"from\", so the\n",
    "### \"to\" processing needs to be a little more complicated\n",
    "# uses counter for iterating through, duplicates process for cc_emails\n",
    "#   does not seem very pythonic, but maybe clearest method\n",
    "if to_emails:\n",
    "    ctr = 0  # counter for iterating through, perhaps not pythonic\n",
    "    while not to_poi and ctr < len(to_emails):\n",
    "        if to_emails[ctr] in poi_email_list:\n",
    "            to_poi = True\n",
    "        ctr += 1\n",
    "```\n",
    "\n",
    "### Useful git code snippets\n",
    "- `git reset --soft HEAD~`\n",
    "    - Leaves working tree as it was before git commit\n",
    "\n",
    "\n",
    "### Techniques\n",
    "\n",
    "\n",
    "### IPython functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Selection?\n",
    "\"Make everything as simple as possible, but no simpler\" - Albert Einstein\n",
    "\n",
    "Two major things aspects:\n",
    "1. Select best features, leaving out unecessary data\n",
    "- Adding new features to explore data, using intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of exploring data with Enron corpus example\n",
    "- Quiz 1: Coding up a new feature\n",
    "- Quiz 2: Scaling this feature\n",
    "\n",
    "### Steps in Katie's process:\n",
    "- Use her human intuition\n",
    "    - Quiz 1: POIs might send other POIs emails more often than the general population\n",
    "    - Quiz 2: Feature scaling might provide more useful visualization\n",
    "- Code up the new feature \n",
    "    - Quiz 1: Calculate aggregate of emails sent from POIs to each person\n",
    "    - Quiz 2: Scale by total number of messages to and from that person\n",
    "- Visualize\n",
    "    - Quiz 1: See scatter plot below to check whether feature gives discriminating power between POIs and non-POIs.\n",
    "- Repeat\n",
    "    - Improve preceeding parts of process\n",
    "    - Zero in on the feature that would be most helpful\n",
    "    \n",
    "__Visualizations:__\n",
    "\n",
    "_Quiz 1:_ Not very useful without scaling\n",
    "![quiz1 scatterplot](lesson_11_images/quiz1_scatter.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a buggy feature\n",
    "When Katie was working on the Enron POI identifier, she engineered a feature that identified when a given person was on the same email as a POI. So for example, if Ken Lay and Katie Malone are both recipients of the same email message, then Katie Malone should have her \"shared receipt\" feature incremented. If she shares lots of emails with POIs, maybe she's a POI herself.\n",
    "\n",
    "Here's the problem: there was a subtle bug, that Ken Lay's \"shared receipt\" counter would also be incremented when this happens. And of course, then Ken Lay always shares receipt with a POI, because he is a POI. So the \"shared receipt\" feature became extremely powerful in finding POIs, because it effectively was encoding the label for each person as a feature.\n",
    "\n",
    "We found this first by being suspicious of a classifier that was always returning 100% accuracy. Then we removed features one at a time, and found that this feature was driving all the performance. Then, digging back through the feature code, we found the bug outlined above. We changed the code so that a person's \"shared receipt\" feature was only incremented if there was a different POI who received the email, reran the code, and tried again. The accuracy dropped to a more reasonable level.\n",
    "\n",
    "We take a couple of lessons from this:\n",
    "- Anyone can make mistakes--be skeptical of your results!\n",
    "- 100% accuracy should generally make you suspicious. Extraordinary claims require extraordinary proof.\n",
    "- If there's a feature that tracks your labels a little too closely, it's very likely a bug!\n",
    "- If you're sure it's not a bug, you probably don't need machine learning--you can just use that feature alone to assign labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring features\n",
    "### Reasons to ignore a feature\n",
    "- Too much noise, hard to dinstinguish whether it is reliably measuring what you want it to be measuring, i.e. data is not accurate/reliable enough.\n",
    "- Causes overfitting for some reason\n",
    "- Highly correlated with/strong related to a feature that is already present, breaking the model because mathematics stops working.\n",
    "- Unecessarily slows down training/testing process when feature is clearly not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features != Information\n",
    "__definition:__ Features vs. Information\n",
    "- A feature is a characteristic of particular data point that is attempting to _access_ information\n",
    "    - In general, we want bare minimum number of features that give as much information as possible\n",
    "- Can think of features as quantity vs. information as quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection tools in sklearn\n",
    "- Feature reduction a.k.a dimensionality reduction\n",
    "- Very important to be skeptical of features, esp. with high _dimensionality data_\n",
    "- In example in tools/email_preprocess.py described below, 90% of features we ignored with:\n",
    "    - insignificant impact on the classifier's accuracy, and \n",
    "    - performance improved in terms of the _time complexity_ of the classifier algorithm.\n",
    "\n",
    "### Univariate Feature Selection\n",
    "There are several go-to methods of automatically selecting your features in sklearn. Many of them fall under the umbrella of univariate feature selection, which treats each feature independently and asks how much _power_ it gives you in classifying or regressing.\n",
    "\n",
    "There are two big univariate feature selection tools in sklearn:\n",
    "- `SelectPercentile` and `SelectKBest`. \n",
    "- The difference is pretty apparent by the names:\n",
    "    - SelectPercentile selects the X% of features that are most powerful (where X is a parameter)\n",
    "    - SelectKBest selects K number of features that are most powerful (where K is a parameter).\n",
    "\n",
    "A clear candidate for feature reduction is text learning, since _the data has such high dimension_. We actually did feature selection in the Sara/Chris email classification problem during the first few mini-projects; you can see it in the code in tools/email_preprocess.py:\n",
    "```\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "```\n",
    "...\n",
    "\n",
    "```\n",
    "### feature selection, because text is super high dimensional and \n",
    "### can be really computationally chewy as a result\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features_train_transformed, labels_train)\n",
    "features_train_transformed = selector.transform(\n",
    "    features_train_transformed).toarray()\n",
    "features_test_transformed  = selector.transform(\n",
    "    features_test_transformed).toarray()\n",
    "```\n",
    "\n",
    "### Feature Selection in TfIdf Vectorizer\n",
    "- NOTE: Usually not a good idea to mix univariate feature selection and Feature Selection with TfIdf Vectorizer parameter `max_df`\n",
    "\n",
    "Example from tools/email_preprocess.py:\n",
    "```\n",
    "### text vectorization--go from strings to lists of numbers\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                               stop_words='english')\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "features_test_transformed  = vectorizer.transform(features_test)\n",
    "```\n",
    "\"`max_df=0.5`\" parameter means that words with a document frequency of more than 0.5 will be removed.\n",
    "- i.e. words that occur in more than 50% of the documents are not included as features\n",
    "- used because words that are probably too common to be very 'powerful', does not provide 'access' to information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, And Number Of Features\n",
    "__definition:__ Bias vs. Variance\n",
    "- High bias algorithm pays _too little_ attention to data, not effected much by data (i.e. oversimplified)\n",
    "    - characterized by _high error_ on training set, i.e. low r<sup>2</sup> / large SSE or sum of squared (residual) errors\n",
    "    - _common when:_ too few features used in model\n",
    "- High variance algorithm pays _too much_ attention to data, does not generalize well, i.e. it overfits to the data\n",
    "    - characterized by _much higher error_ on test set than training (some variance between the two is expected).\n",
    "    - _common when:_ carefully minimized SSE, with too many features i.e. overfit to data\n",
    "\n",
    "__trade-off:__ Quality of model vs. no. of features:\n",
    "\n",
    "Goal is to balance:\n",
    "- the performance/accuracy of the model on the training data\n",
    "    - (without overfitting)\n",
    "- with as few features as possible.\n",
    "    - (while maintaining algorithm's performance)\n",
    "    \n",
    "### Visualizing Overfitting\n",
    "#### An Overfit Regression\n",
    "- Blue points are training data\n",
    "- Red points are test data\n",
    "![An Overfit Regression](lesson_11_images/overfit_regression.png)\n",
    "\n",
    "### Regularization\n",
    "__definition:__ automatically penalizing extra features in model\n",
    "\n",
    "![Regularization graph](lesson_11_images/regularization_graph.png)\n",
    "\n",
    "#### Lasso Regression (type of regularized regression)\n",
    "`sklearn.linear_model`.Lasso [Documentation][lasso_doc] and [User Guide][lasso_user]\n",
    "- Mathematical optimization of the bias-variance trade-off\n",
    "- Minimizes SSE like basic regression\n",
    "- But also minimizes term (penalty parameter * coefficients of regression)\n",
    "    - coefficients of regression describe/are related to no. of features\n",
    "    - if feature does not add enough precision, it's coefficient is set to 0\n",
    "        - simpler algorithm with this method since it can run through the formula in-place.\n",
    "- This means that any loss from an additional feature must be offset by the gain in precision by adding that feature.\n",
    "\n",
    "   \n",
    "![Lasso regression formula](lesson_11_images/lasso_regression_formula.png)\n",
    "![Lasso regression penalty method](lesson_11_images/lasso_regression_penalty_method.png)\n",
    "\n",
    "[lasso_doc]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "[lasso_user]: http://scikit-learn.org/stable/modules/linear_model.html#lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-project! on feature selection\n",
    "Katie explained in a video a problem that arose in preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here.\n",
    "\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.\n",
    "If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?\n",
    "\n",
    "- Answer: Pretty low (accuracy on training set would be way higher)\n",
    "\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. How many training points are there, according to the starter code?\n",
    "\n",
    "- Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
