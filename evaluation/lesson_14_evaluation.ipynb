{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenotes (definitions, code snippets, resources, etc.)\n",
    "- Note on data structure: list\n",
    "    - empty list has a truth value of false\n",
    "- [Feature Selection with scikit-learn for intro_to_ml](http://napitupulu-jon.appspot.com/posts/feature-selection-ud120.html)\n",
    "    - Looks very helpful for copying notes, course materials\n",
    "    - Investigate meaning of `# %%writefile new_enron_feature.py` inserted at top of edited studentMain.py module\n",
    "\n",
    "### Latex\n",
    "To use Python to display Latex equations etc.:\n",
    "```python\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "## Accuracy\n",
    "__formula:__ \n",
    "\n",
    "$\\text{accuracy} = \\frac{\\text{no. of data points labeled corrected}}{ \\text{all data points}}$\n",
    "\n",
    "Shortcoming of accuracy measurement:\n",
    "- Not good for skewed classes (i.e. most of the data under one label)\n",
    "    - because demoninator will be small, so measurement not trustworthy\n",
    "- Not suited to particular labeling requirements i.e. need to err on one label over the other.\n",
    "    - different performance metrics can focus on different types of errors (false positives, false negatives).\n",
    "    \n",
    "## Confusion Matrices\n",
    "Exmaple of Confusion Matrix analysis with Decision Tree:\n",
    "![confusion matrix example](lesson_14_images/confusion_matrix_example.png)\n",
    "\n",
    "__formulas:__ \n",
    "- $\\text{accuracy} = \\frac{\\text{true positives + true negatives}}{ \\text{all data points}}$\n",
    "\n",
    "in contrast to:\n",
    "- $\\text{Recall(x)} = \\frac{\\text{data points correctly labeled as x}}{ \\text{total data points actually x}} = \\frac{\\text{true positives}}{ \\text{false negatives + true positives}}$\n",
    "    - i.e. of all the data points actually labeled x, how many are accurately predicted.\n",
    "    - this gives us greater confidence with negative predictions.\n",
    "    - i.e. the probability of the data point being correctly labeled as x\n",
    "    - i.e. that when the alg assigns a label on data point with actual label x, that data point is actually x\n",
    "- $\\text{Precision(x)} = \\frac{\\text{data points correctly labeled as x}}{ \\text{total data points predicted x}} = \\frac{\\text{true positives}}{ \\text{false positives + true positives}}$\n",
    "    - i.e. of all the predicted positives, how many are true?\n",
    "    - this gives us greater confidence with positive predictions.\n",
    "    - i.e. the probability of the label being correctly assigned\n",
    "    - i.e. that when the alg assigns label x, that that data point is actually x\n",
    "- $F_{1} = \\frac{\\text{precision $\\cdot$ recall}}{\\text{precision + recall}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mini-project! Applying Metrics to Your POI Identifier\n",
    "[not labeled mini-project in course]\n",
    "\n",
    "Go back to your code from the last lesson, where you built a simple first iteration of a POI identifier using a decision tree and one feature. Copy the POI identifier that you built into the skeleton code in evaluation/evaluate_poi_identifier.py. Recall that at the end of that project, your identifier had an accuracy (on the test set) of 0.724. Not too bad, right? Let’s dig into your predictions a little more carefully.\n",
    "\n",
    "From Python 3.3 forward, a change to the order in which dictionary keys are processed was made such that the orders are randomized each time the code is run. This will cause some compatibility problems with the graders and project code, which were run under Python 2.7. To correct for this, add the following argument to the featureFormat call on line 25 of evaluate_poi_identifier.py:\n",
    "\n",
    "sort_keys = '../tools/python2_lesson14_keys.pkl'\n",
    "\n",
    "This will open up a file in the tools folder with the Python 2 key order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.724137931034\n",
      "No. POIs predicted in test set:  4\n",
      "No. of true positives:  0\n"
     ]
    }
   ],
   "source": [
    "# Final model from L13\n",
    "from evaluate_poi_identifier import *\n",
    "\n",
    "### it's all yours from here forward!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score    \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print \"Accuracy score: \", accuracy_score(labels_test, pred)\n",
    "print \"No. POIs predicted in test set: \", len([x for x in pred if x == 1])\n",
    "print \"No. of true positives: \", (\n",
    "    len([i for i, j in zip(labels_test, pred) if i and j == 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in sklearn.metrics to compute those quantities.\n",
    "\n",
    "What’s the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:  0.0\n",
      "Recall score:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print \"Precision score: \", precision_score(labels_test, pred)\n",
    "print \"Recall score: \", recall_score(labels_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
